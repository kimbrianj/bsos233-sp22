{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 11: Predicting Low Birthweight \n",
    "\n",
    "Please complete this lab by providing answers in cells after the question. Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**.\n",
    "\n",
    "This assignment is due by **11:59pm on Thursday, April 28**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "\n",
    "# These lines do some fancy plotting magic.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is for linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# This is for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For getting results from the test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Carolina Births Data\n",
    "\n",
    "In this lab, we will work with a dataset of births in North Carolina. A series of variables were collected, most notably the smoking habit of the mother as well as the birthweight of the baby. We are interested in what factors are associated with a low birthweight. We'll first look at predicting `birthweight` as a numerical variable, then use the categorical `lowbirthweight` and try to predict low birthweight status using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbirths = Table.read_table('ncbirths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbirths.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to work with the data a little bit to be able to fit a model using categorical data. The `sklearn` package only allows for categorical variables that have been changed into dummy variables (that is, 0/1 variables). So, we're going to need to create new variables that contain the same information, except as numbers. Luckily, True/False maps onto 1/0, so we can just do comparisons. We'll use a lot of these variables later, so let's do some cleaning now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbirths_dummy = ncbirths.with_columns('premature', ncbirths.column('premie') == 'premie', # True if premature, False if not\n",
    "                     'female', ncbirths.column('gender') == 'female', # True if female, False if not\n",
    "                     'smoker', ncbirths.column('habit') == 'smoker', # True if smoker, False if not\n",
    "                     'label', ncbirths.column('lowbirthweight') == 'low') # Our outcome. True if low birthweight, False if not\n",
    "\n",
    "\n",
    "ncbirths_dummy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant rows now\n",
    "ncbirths_with_dummies = ncbirths_dummy.drop('premie', 'gender', 'habit','lowbirthweight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 1. Using `ncbirths_with_dummies`, change the `whitemom` and `mature` variables so that they are True/False values. Add a column called `younger` which is True if `mature` is \"younger mom\" and False otherwise, and a column called `white` which is True if `whitemom` is \"white\" and False otherwise. Assign this to `ncbirths_clean`. Make sure to remove the redundant variables (`whitemom` and `mature`).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's try fitting a logistic regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Logistic Regression\n",
    "\n",
    "A logistic regression is essentially the same as a linear regression, except we have a binary (two category) categorical variable instead of a numerical variable as the outcome, and we get predicted probabilities out of the predictions. The way we fit a logistic regression model is essentially the same as a linear regression model with `sklearn`. We set up a `LogisticRegression` object first, then we use `fit` to provide it the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Define predictor and outcome\n",
    "predictor = ncbirths_clean.select('mage', 'weeks', 'female', 'smoker').rows\n",
    "outcome = ncbirths_clean.column('label')\n",
    "\n",
    "# Fit the model\n",
    "logit.fit(X = predictor, y = outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the coefficients and intercept the same way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 2. Write out the form of the equation in the logistic regression model that we just ran. You can just use `logit(birthweight status)` as the outcome instead of writing out the logit transformation.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 3. Focusing on just the sign of the coefficients, interpret the relationship between each of the predictors and whether the baby had a low birthgweight.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition for Logistic Regression\n",
    "\n",
    "We are treating the outcome variable (in this case, low birthweight status) as a 0/1 variable. The graph below shows the relationship between number of weeks of pregnancy and the actual outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_and_weight = Table().with_columns('Weeks', ncbirths_clean.column('weeks'),\n",
    "                                       'Low Birthweight Status', ncbirths_clean.column('label').astype(int))\n",
    "weeks_and_weight.scatter('Weeks', 'Low Birthweight Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in gold the predicted probabilities at the different number of weeks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_and_weight = Table().with_columns('Weeks', ncbirths_clean.column('weeks'),\n",
    "                                       'Low Birthweight Status', ncbirths_clean.column('label').astype(int))\n",
    "weeks_and_weight.scatter('Weeks', 'Low Birthweight Status')\n",
    "plt.scatter(ncbirths_clean.column('weeks'), logit.predict_proba(predictor)[:,1], lw=1, color='gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit transformation makes it so that even though there is a linear relationship between the (transformed) outcome and the predictors, all predicted probabilities are between 0 and 1, and the relationship between weeks and low birthweight status can clearly be seen here, with higher number of weeks corresponding to lower probability of low birthweight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 4. Try adding another predictor to the logistic regression model and running it again. Make sure you do not use `weight`, since that is exactly what the `lowbirthweight` variable is based on. How does your model change? What are the coefficients? How would you interpret the coefficients for the variable you added?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using Logistic Regression\n",
    "\n",
    "To do predictions, we use the `predict_proba` method rather than the `predict` method. This will give us a set of two probabilities for each row. The first represents the probability of a 0, while the second represents the probability of a 1. Here, we are more interested in predicting the probability that a baby will have a low birthweight so that we can provide some sort of help or care in advance. So, we'll just try to find the ones with the highest probability. \n",
    "\n",
    "In this example, we simply use the same dataset (`ncbirths`) to make predictions, but in reality, you'd use a different dataset. In the next section, we'll go over something called training and testing data that can help improve your models and their performance on future data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for each element in original dataset\n",
    "# Each row has probabilities for 0 and 1\n",
    "# Should add up to one on each row.\n",
    "logit.predict_proba(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract predicted probability of being low birthweight.\n",
    "# The [:,1] means that we want everything from the rows, and only the column with index 1 (so, the second column)\n",
    "lowbirthweight_probability = logit.predict_proba(predictor)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_column('Low Birthweight Predicted Probability',lowbirthweight_probability).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 5. What would be the predicted probability of a baby being lowbirthweight according to our model if the mother's age was 30, the pregnancy lasted 36 weeks, the mother was not a smoker, and the baby was female?**</font>\n",
    "\n",
    "**Hint:** You'll need to create a Table to use with `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train and Test\n",
    "\n",
    "What if we wanted to get a sense for how our model might perform when given new data? We can split our data into train and test sets, build our model using the train, and assess our model using test.\n",
    "\n",
    "First, let's start by taking a sample of our data. We'll use 80% of the data to build our model, then using the remaining 20% to see how it is doing. To do this, we first shuffle our dataset (the cleaned version), then take the first 80% of the rows as `train`. Then, we assign the rest to `test`. The shuffling is to make sure that we are doing a random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_births = ncbirths_clean.sample(with_replacement = False)\n",
    "\n",
    "nrows = ncbirths.num_rows\n",
    "ntrain = int(nrows*.8) # 80% used to train\n",
    "\n",
    "train = shuffled_births.take(np.arange(ntrain))\n",
    "test = shuffled_births.take(np.arange(ntrain,nrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take the same exact steps as we did earlier, except using the `train` object to build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 6. Fit a logistic regression model using `train` with `mage`, `weeks`, `female`, and `smoker` as the predictors. What are the coefficients? How does it differ from fitting the model using the full data?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use `predict` to predict for the cases in `test` and see how our model did. The `predict` function takes the predicted probabilities and assigns a value of 1 as the prediction if the probability is above 0.5 and 0 if it is below 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = logit.predict(test.select('mage', 'weeks', 'female', 'smoker').rows)\n",
    "confusion_matrix(test.column('label'), test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns represent predictions and the rows represent actual values, so the top left is true negatives, the bottom right is true positives, the top right is false positives, and the bottom left is false negatives. We can calculate the accuracy of our model using the `accuracy_score` function. This takes the diagonal values and divides by the total number of cases to give us how often we were correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test.column('label'), test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, remember that `predict` by default uses 0.5 as the cutoff point for predicting 0 or 1. We might actually want a different cutoff. For example, if we think a 10% chance is a high enough risk that we want to make sure we flag the baby as possibly being low birthweight, then we might want to set a cutoff value of 0.1 instead. This would make it so that we are predicting more babies as low birthweight. We can do this using `predict_proba` and setting a cutoff manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = logit.predict_proba(test.select('mage', 'weeks', 'female', 'smoker').rows)[:,1] > 0.1\n",
    "confusion_matrix(test.column('label'), test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test.column('label'), test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the true positives go up, but so do the false positives. That is, we are predicting more babies to be low birthweight, which is able to catch more cases, but we also increase the number of times we incorrectly predict a baby is low birthweight. This is a tradeoff -- the lower we set the threshold, the fewer false negatives we get, but also the more false positives as well.\n",
    "\n",
    "You probably observed a lower accuracy for one of the models. This doesn't necessarily mean we would always prefer the higher accuracy predictions. For example, we might be ok with more errors as long as we are able to catch all of the babies who might be at risk of being low birthweight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 7. Try a few different values for the cutoff other than 0.1 and 0.5. How does the number of false positives and false negatives change? How does the accuracy change? What do you think would be the preferred set of predictions to use?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
