{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: APIs and Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Text Analysis on University of Maryland Patents\n",
    "\n",
    "We are able to use the [Patentsview API](https://www.patentsview.org/api/query-language.html) to pull data from patents awarded to inventors at University of Maryland, including the abstract from each of the patents. Suppose we wanted to know what types of patents were awarded to UMD. We can look at the information from the abstracts and read through them, but this would take a very long time since there are almost 1,300 abstracts. Instead, we can use **text analysis** to help us.\n",
    "\n",
    "However, as-is, the text from abstracts can be difficult to analyze. We aren't able to use traditional statistical techniques without some heavy data manipulation, because the text is essentially a categorical variable with unique values for each patent. We need to basically break it apart and clean the data before we apply our data analysis techniques. \n",
    "\n",
    "In this notebook, we will go through the process of cleaning and processing the text data to prepare it for **topic modeling**, which is the process of automatically assigning topics to individual **documents** (in this case, an individual document is an abstract from a patent). We will use a technique called **Latent Dirichlet Allocation** as our topic modeling technique, and try to determine what sorts of patents were awarded to University of Maryland."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interacting with websites and web-APIs\n",
    "import requests # easy way to interact with web sites and services\n",
    "\n",
    "# data manipulation\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Text Analysis Tools\n",
    "\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs with Python\n",
    "\n",
    "APIs (application programming interfaces) are hosted on web servers. When you type www.google.com in your browser's address bar, your computer is actually asking the www.google.com server for a webpage, which it then returns to your browser. APIs work much the same way, except instead of your web browser asking for a webpage, your program asks for data. This data is usually returned in JSON format. \n",
    "\n",
    "To retrieve data, we make a request to a webserver. The server then replies with our data. In Python, we'll use the `requests` library to do this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Patent Data about University of Maryland\n",
    "\n",
    "We will use the `request` package to retrieve information about the patents that have been granted to inventors at University of Maryland, using the PatentsView API. This notebook goes over using the `request` package to get the data, as well as putting that data into a form that is usable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the request package work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to understand what information can be accessed from the API. We use an example of the **PatentsView API** (www.patentsview.org) to make the API call and check the information we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About PatentsView API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PatentsView platform is built on data derived from the US Patent and Trademark Office (USPTO) bulk data to link inventors, their organizations, locations, and overall patenting activity. The PatentsView API provides programmatic access to longitudinal data and metadata on patents, inventors, companies, and geographic locations since 1976. \n",
    "\n",
    "To access the API, we use the `request` function. In order to tell Python what to access, we need to specify the url of the API endpoint.\n",
    "\n",
    "PatentsView has several API endpoints. An endpoint is a server route that is used to retrieve different data from the API. You can think of the endpoints as just specifying what types of data you want. Examples of PatentsView API endpoints are shown here: http://www.patentsview.org/api/doc.html\n",
    "\n",
    "Many times, we need to request a key from the data provider in order to access an API. For example, if you wanted to access the Twitter API, then you would need to get a Twitter developer account and access token (see [https://developer.twitter.com/en/docs/basics/authentication/overview/oauth](https://developer.twitter.com/en/docs/basics/authentication/overview/oauth)). Currently no key is necessary to access the PatentsView API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Request\n",
    "When you ping a website or portal for information this is called making a request. That is exactly what the `requests` library has been designed to do. However, we need to provide a query URL according to the format defined by PatentsView. The details on how to do that is explained [at this link.](https://www.patentsview.org/api/query-language.html)\n",
    "\n",
    "Following the directions detailed in the link above, let's build our first query URL.\n",
    "\n",
    "**Query String Format**\n",
    "\n",
    "The query string is always a single JSON object: **{`<field>`:`<value>`}**, where `<field>` is the name of a database field and `<value>` is the value the field will be compared to for equality (Each API Endpoint section contains a list of the data fields that can be selected for inclusion in output datasets).\n",
    "\n",
    "We use the following base URL for the Patents Endpoint:\n",
    "\n",
    "**Base URL**: `https://api.patentsview.org/patents/query?q={criteria}`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task example: Pull patents for University of Maryland\n",
    "\n",
    "In this example, we will only pull patents from one organization: University of Maryland. Let's go to the Patents Endpoint (http://www.patentsview.org/api/patent.html) and find the appropriate field for the organization's name. Based on looking at the APID documentation, we can see that the variable that we need is called `\"assignee_organization\"` (organization name, if assignee is organization).\n",
    "\n",
    "> _Note_: **Assignee**: the name of the entity - company, foundation, partnership, holding company or individual - that owns the patent. In this example we are looking at universities (organization-level).\n",
    "\n",
    "We will pull from the API using a step-by-step process:\n",
    "- Build the query\n",
    "- Get the response\n",
    "- Check the response code\n",
    "- Get the content\n",
    "- Convert to table\n",
    "\n",
    "By the end, we should have data about patents that we can work with using the tools we've already learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Build the URL query \n",
    "\n",
    "Let's build our first URL query by combining the base url with one criterion (name of the `assignee_organization`). This is based on the directions detailed [at this link.](https://www.patentsview.org/api/query-language.html)\n",
    "\n",
    "To build it up, we start with the base url (`http://www.patentsview.org/api/patents/query?q=`) and add the various criteria that we want. For example, we use `'q={\"assignee_organization\":\"University of Maryland, College Park\"}'` to set the organization to University of Maryland, College Park."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.patentsview.org/patents/query?q={\"assignee_organization\":\"University of Maryland, College Park\"}&f=[\"patent_title\",\"patent_year\", \"patent_type\", \"patent_abstract\"]&o={\"per_page\":1300}\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://api.patentsview.org/patents/query?'\n",
    "organization = 'q={\"assignee_organization\":\"University of Maryland, College Park\"}'\n",
    "variables = '&f=[\"patent_title\",\"patent_year\", \"patent_type\", \"patent_abstract\"]'\n",
    "options = '&o={\"per_page\":1300}'\n",
    "\n",
    "url = base_url + organization + variables + options\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Get the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the response using the URL defined above, using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)  # Get response from the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Check the Response Code\n",
    "\n",
    "Before you can do anything with a website or URL in Python, it’s a good idea to check the current status code of said portal.\n",
    "\n",
    "The following are the response codes for the PatentsView API:\n",
    "\n",
    "`200` - the query parameters are all valid; the results will be in the body of the response\n",
    "\n",
    "`400` - the query parameters are not valid, typically either because they are not in valid JSON format, or a specified field or value is not valid; the “status reason” in the header will contain the error message\n",
    "\n",
    "`500` -  there is an internal error with the processing of the query; the “status reason” in the header will contain the error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code  # Check the status code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good to go. Now let's get the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Get the Content\n",
    "After a web server returns a response, you can collect the content you need by converting it into a JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is a way to encode data structures like lists and dictionaries to strings that ensures that they are easily readable by machines. JSON is the primary format in which data is passed back and forth to APIs, and most API servers will send their responses in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to use this data, but it's a bit hard to in the current JSON format. We want to essentially take the information that is in the `patents` field within the dictionary and create a Table out of it. To do that, we'll use a trick called **list comprehension** that will make our lives much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Comprehension\n",
    "\n",
    "List comprehension is kind of like a compact `for` loop inside a list. You use it to generate a list of values with certain characteristics. For example, if we wanted to create a list with values from 0 to 9, we could use the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in np.arange(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Converting JSON into a Table\n",
    "\n",
    "Now let's convert the JSON into a `Table`. To do this, let's first examine how the JSON looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>patent_title</th> <th>patent_year</th> <th>patent_type</th> <th>patent_abstract</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Methods for recovery of leaf proteins                       </td> <td>2018       </td> <td>utility    </td> <td>A novel method for processing soluble plant leaf protein ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Systems, methods, and devices for health monitoring of a ...</td> <td>2018       </td> <td>utility    </td> <td>A health monitoring device includes an ultrasound source ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Sparse decomposition of head related impulse responses w ...</td> <td>2018       </td> <td>utility    </td> <td>This application describes methods of signal processing  ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Flapping wing aerial vehicles                               </td> <td>2018       </td> <td>utility    </td> <td>An autonomous flapping wing aerial vehicle can have a ve ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Compositions and vaccines comprising vesicles and method ...</td> <td>2018       </td> <td>utility    </td> <td>The disclosure relates to compositions, pharmaceutical c ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (393 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patent_title = [a['patent_title'] for a in json['patents']]\n",
    "patent_year = [a['patent_year'] for a in json['patents']]\n",
    "patent_type = [a['patent_type'] for a in json['patents']]\n",
    "patent_abstract = [a['patent_abstract'] for a in json['patents']]\n",
    "\n",
    "patents = Table().with_columns('patent_title', patent_title,\n",
    "                               'patent_year', patent_year,\n",
    "                              'patent_type', patent_type,\n",
    "                               'patent_abstract', patent_abstract)\n",
    "patents.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out just the abstracts as an array to use for our text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = patents.column('patent_abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "We are going to apply topic modeling, an unsupervised learning method, to our corpus to find the high-level topics in our corpus. Through this process, we'll discuss how to clean and preprocess our data to get the best results. Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on a common modeling approach called Latent Dirichlet Allocation (LDA). \n",
    "\n",
    "To use topic modeling, we first have to assume that topics exist in our corpus, and that some small number of these topics can \"explain\" the corpus. Topics in this context refer to words from the corpus, in a list that is ranked by probability. A single document can be explained by multiple topics. For instance, an article on net neutrality would fall under the topic \"technology\" as well as the topic \"politics.\" The set of topics used by a document is known as the document's allocation, hence, the name Latent Dirchlet Allocation, each document has an allocation of latent topics allocated by Dirchlet distribution. \n",
    "\n",
    "We will use topic modeling in order to determine what types of inventions have been produced at the University of Maryland. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text Data for Natural Language Processing (NLP)\n",
    "\n",
    "The first important step in working with text data is cleaning and processing the data, which includes (but is not limited to):\n",
    "\n",
    "- forming a corpus of text\n",
    "- stemming and lemmatization\n",
    "- tokenization\n",
    "- removing stop-words\n",
    "- finding words co-located together (N-grams)\n",
    "\n",
    "\n",
    "\n",
    "The ultimate goal is to transform our text data into a form an algorithm can work with, because a document or a corpus of text cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with certain fixed sizes, and can't handle documents, which are basically sequences of symbols with variable length. We will be transforming our text corpus into a *bag of n-grams* to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization - Distilling text data\n",
    "\n",
    "We want to process our text through *stemming and lemmatization*, or replacing words with their root or simplest form. For example \"systems,\" \"systematic,\" and \"system\" are all different words, but we can replace all these words with \"system\" without sacrificing much meaning. \n",
    "\n",
    "- A **lemma** is the original dictionary form of a word (e.g. the lemma for \"lies,\" \"lied,\" and \"lying\" is \"lie\").\n",
    "- The process of turning a word into its simplest form is **stemming**. There are several well known stemming algorithms -- Porter, Snowball, Lancaster -- that all have their respective strengths and weaknesses.\n",
    "\n",
    "In this notebook, we'll use the Snowball Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of how a Stemmer works:\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem(\"lying\"))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 2. Using the format of the examples above, try stemming the word `systematically` and `systemic`.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "For some purposes, we might want to preserve punctuation. For example, if we wanted to be able to detect sentiment of text, we might want to keep exclamation points, because they signify something about the text. For our purposes, however, we will simply strip the punctuation so that it does not affect our analysis. To do this, we use the `string` package, creating a translator that takes any string and \"translates\" it into a string without any punctuation.\n",
    "\n",
    "An example using the first abstract in our corpus is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create translator\n",
    "# This maps all punctuation to an empty space\n",
    "translator=str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "# After\n",
    "abstracts[0].translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "We want to separate text into individual tokens (generally individual words). To do this, we'll first write a function that takes a string and splits it up into indiviudal words. We'll do the whole process of removing punctuation, stemming, and tokenizing all in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    translator=str.maketrans(string.punctuation, ' '*len(string.punctuation)) # translator that replaces punctuation with empty spaces\n",
    "    return [stemmer.stem(i) for i in text.translate(translator).split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenize` function actually does several things at the same time. First, it removes any punctuation using the `translate` method. Then, the `split` method breaks it apart into individual words. Then, using `stemmer.stem`, it creates a list of the stemmed versions of each of those individual words. \n",
    "\n",
    "Let's take a look at an example of how this works using the first abstract in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(abstracts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we get out of it is something called a **bag of words**. This is a list of all of the words that are in the abstract, cleaned of all punctuation and stemmed. The paragraph is now represented as a vector of individual words rather than as one whole entity. \n",
    "\n",
    "We can apply this to each abstract in our corpus using `CountVectorizer`. This will not only do the tokenizing, but it will also count any duplicates of words and create a matrix that contains the frequency of each word. This will be quite a large matrix (number of columns will be number of unique words), so it outputs the data as a sparse matrix.\n",
    "\n",
    "Similar to how we fit models using `sklearn`, we will first create the `vectorizer` object (you can think of this like a model object), and then fit it with our abstracts. This should give us back our overall corpus bag of words, as well as a list of features (that is, the unique words in all the abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,1),\n",
    "                            strip_accents='unicode',\n",
    "                            min_df = 0.05,\n",
    "                            max_df = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format looks a bit weird, but it's showing the `(row, column)` and corresponding value associated with it. Everything else is a `0`. Let's take a look at the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our bag of words, we can start using it for models such as Latent Dirichlet Allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a statistical model that generates groups based on similarities. This is an example of an **unsupervised machine learning model**. That is, we don't have any sort of outcome variable -- we're just trying to group the abstracts into rough categories.\n",
    "\n",
    "Let's try fitting an LDA model. The way we do it is very similar to the models we've fit before from `sklearn`. We first create a `LatentDirichletAllocation` object, then fit it using our corpus bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(learning_method='online') \n",
    "\n",
    "doctopic = lda.fit_transform( bag_of_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:5]\n",
    "    keywords = ', '.join( features[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look very helpful! There are way too many common words in the corpus, such as 'a', 'of', and so on. We need to remove them, because they don't actually have any interesting information about the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing meaningless text - Stopwords\n",
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download most current stopwords\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize stop words to match\n",
    "eng_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,1),\n",
    "                            strip_accents='unicode',\n",
    "                            stop_words=eng_stopwords,\n",
    "                            min_df = 0.05,\n",
    "                            max_df = 0.95)\n",
    "\n",
    "# Creating bag of words\n",
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "doctopic = lda.fit_transform( bag_of_words )\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:5]\n",
    "    keywords = ', '.join( features[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 3. What does it look like the topics of the abstracts are like? Are some categories harder to distinguish than others?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams - Adding context by creating N-grams\n",
    "\n",
    "Obviously, reducing a document to a bag of words means losing much of its meaning - we put words in certain orders, and group words together in phrases and sentences, precisely to give them more meaning. If you follow the processing steps we've gone through so far, splitting your document into individual words and then removing stopwords, you'll completely lose all phrases like \"kick the bucket,\" \"commander in chief,\" or \"sleeps with the fishes.\" \n",
    "\n",
    "One way to address this is to break down each document similarly, but rather than treating each word as an individual unit, treat each group of 2 words, or 3 words, or *n* words, as a unit. We call this a \"bag of *n*-grams,\" where *n* is the number of words in each chunk. Then you can analyze which groups of words commonly occur together (in a fixed order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,2), # Allow for bigrams\n",
    "                            strip_accents='unicode',\n",
    "                            stop_words=eng_stopwords,\n",
    "                            min_df = 0.05,\n",
    "                            max_df = 0.95)\n",
    "\n",
    "# Creating bag of words\n",
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "doctopic = lda.fit_transform( bag_of_words )\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:10]\n",
    "    keywords = ', '.join( features[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Weighting terms based on frequency\n",
    "\n",
    "A final step in cleaning and processing our text data is **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is based on the idea that the words (or terms) that are most related to a certain topic will occur frequently in documents on that topic, and infrequently in unrelated documents.  TF-IDF re-weights words so that we emphasize words that are unique to a document and suppress words that are common throughout the corpus by inversely weighting terms based on their frequency within the document and across the corpus.\n",
    "\n",
    "Let's look at how to use TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + ['invent', 'produce', 'method', 'use', 'first', 'second']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,2),\n",
    "                            strip_accents='unicode',\n",
    "                            stop_words=full_stopwords,\n",
    "                            min_df = 0.05,\n",
    "                            max_df = 0.95)\n",
    "# Creating bag of words\n",
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# Use TfidfTransformer to re-weight bag of words \n",
    "transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tfidf = transformer.fit_transform(bag_of_words)\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "doctopic = lda.fit_transform(tfidf)\n",
    "\n",
    "# Displaying the top keywords in each topic\n",
    "ls_keywords = []\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(topic)[::-1][:5]\n",
    "    keywords = ', '.join( features[i] for i in word_idx)\n",
    "    ls_keywords.append(keywords)\n",
    "    print(i, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put this all together in a Table in order to see it more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = Table().with_columns('Patent Title', patents.column('patent_title'),\n",
    "                             ls_keywords[0],doctopic[:,0],\n",
    "                             ls_keywords[1],doctopic[:,1],\n",
    "                             ls_keywords[2],doctopic[:,2],\n",
    "                             ls_keywords[3],doctopic[:,3],\n",
    "                             ls_keywords[4],doctopic[:,4],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 4. Try editing the above code to show a different number of topics and keywords for each topic.**</font>\n",
    "\n",
    "To change the number of topics, you can adjust the following line:\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "\n",
    "and change the `n_components` value to the number of topics that you want.\n",
    "\n",
    "To change the number of keywords that are displayed for each topic, change the following line:\n",
    "\n",
    "    word_idx = np.argsort(topic)[::-1][:5]\n",
    "\n",
    "by adjusting the `5` to the number of keywords you want to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Document Classification\n",
    "\n",
    "Previously, we used topic modeling to infer relationships between social service facilities within the data. That is an example of unsupervised learning: we were looking to uncover structure in the form of topics, or groups of agencies, but we did not necessarily know the ground truth of how many groups we should find or which agencies belonged in which group.  \n",
    "\n",
    "We can also do supervised learning with text data. In supervised learning, we have a *known* outcome or label (*Y*) that we want to produce given some data (*X*), and in general, we want to be able to produce this *Y* when we *don't* know it, or when we *only* have *X*. \n",
    "\n",
    "In order to produce labels we need to first have examples our algorithm can learn from, a \"training set.\" In the context of text analysis, developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. **Document classification** is an example of supervised learning in which want to characterize our documents based on their contents (*X*). A common example of document classification is spam e-mail detection. Another example of supervised learning in text analysis is *sentiment analysis*, where *X* is our documents and *Y* is the state of the author. This \"state\" is dependent on the question you're trying to answer, and can range from the author being happy or unhappy with a product to the author being politically conservative or liberal. Another example is *part-of-speech tagging* where *X* are individual words and *Y* is the part-of-speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for NLP in Python is \n",
    "[Natural Language Processing with Python](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
